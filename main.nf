import groovy.json.JsonOutput

include { gptPromptForText } from 'plugin/nf-gpt'


system_gen_system_prompt = """\
Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.

The prompts you will be generating will be for freeform tasks, such as generating a landing page headline, an intro paragraph, solving a math problem, etc.

In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.

You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.

Most importantly, output NOTHING but the prompt. Do not include anything else in your message."""


ranking_system_prompt = """Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.

You will be provided with the task description, the test prompt, and two generations - one for each system prompt.

Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.

Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.

Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.

Respond with your ranking, and nothing else. Be fair and unbiased in your judgement."""


params.K = 32

params.CANDIDATE_MODEL = 'gpt-4'
params.CANDIDATE_MODEL_TEMPERATURE = 0.9d

params.GENERATION_MODEL = 'gpt-3.5-turbo'
params.GENERATION_MODEL_TEMPERATURE = 0.8d
params.GENERATION_MODEL_MAX_TOKENS = 60

N_RETRIES = 3  // number of times to retry a call to the ranking model if it fails
params.RANKING_MODEL = 'gpt-3.5-turbo'
params.RANKING_MODEL_TEMPERATURE = 0.5d

params.NUMBER_OF_PROMPTS = 5 // this determines how many candidate prompts to generate... the higher, the more expensive, but the better the results will be

description = "Given a prompt, generate a landing page headline."

test_cases = [
        [
            'prompt': 'Promoting an innovative new fitness app, Smartly',
        ],
        [
            'prompt': 'Why a vegan diet is beneficial for your health',
        ],
        [
            'prompt': 'Introducing a new online course on digital marketing',
        ],
        [
            'prompt': 'Launching a new line of eco-friendly clothing',
        ],
        [
            'prompt': 'Promoting a new travel blog focusing on budget travel',
        ],
        [
            'prompt': 'Advertising a new software for efficient project management',
        ],
        [
            'prompt': 'Introducing a new book on mastering Python programming',
        ],
        [
            'prompt': 'Promoting a new online platform for learning languages',
        ],
        [
            'prompt': 'Advertising a new service for personalized meal plans',
        ],
        [
            'prompt': 'Launching a new app for mental health and mindfulness',
        ]
]

def expected_score(r1, r2) {
    return 1 / (1 + 10**((r2 - r1) / 400))
}

def update_elo(r1, r2, score1) {
    def e1 = expected_score(r1, r2)
    def e2 = expected_score(r2, r1)
    return [r1 + params.K * (score1 - e1), r2 + params.K * ((1 - score1) - e2)]
}


def get_generation(prompt, test_case) {
    def messages=[
            ["role": "system", "content": prompt],
            ["role": "user", "content": test_case['prompt']]
    ]

    return gptPromptForText(
                messages,
                model: params.GENERATION_MODEL,
                temperature: params.GENERATION_MODEL_TEMPERATURE,
                maxTokens: params.GENERATION_MODEL_MAX_TOKENS)
}

def get_score(description, test_case, pos1, pos2, ranking_model_name, ranking_model_temperature) {

    def messages=[
            ["role": "system", "content": ranking_system_prompt],
            ["role": "user", "content": """\
Task: ${description.trim()}
Prompt: ${test_case['prompt']}
Generation A: ${pos1}
Generation B: ${pos2}"""]
    ]

    return gptPromptForText(
                messages,
                model: ranking_model_name,
                logitBias: ['32':100, '33':100],
                maxTokens: 1,
                temperature: ranking_model_temperature )
}

List combinations(List list) {
    def pairs = []
    for (int i = 0; i < list.size() - 1; i++) {
        for (int j = i + 1; j < list.size(); j++) {
            pairs << [list[i], list[j]]
        }
    }
    return pairs
}

Map aggregate(Map prompt_ratings, Map r) {
    log.debug ">>> result=$r"
    // # Average the scores
    def score = (r.score1 + r.score2) / 2

    // # Update ELO ratings
    def r1 = prompt_ratings.getOrDefault(r.prompt1, 1200)
    def r2 = prompt_ratings.getOrDefault(r.prompt2, 1200)

    def (s1, s2) = update_elo(r1, r2, score)
    prompt_ratings[r.prompt1] = s1
    prompt_ratings[r.prompt2] = s2

    return prompt_ratings
}

String renderRatings(Map ratings) {
    def sorted = ratings.sort { a, b -> b.value <=> a.value }
    def result = "Rating:   Prompt\n"
    for( def entry : sorted.entrySet() )
        result += "$entry.value:    $entry.key\n\n"
    return result
}

process generate_candidate_prompts {
    input:
    val description
    val test_cases
    val number_of_prompts
    output:
    val prompts

    exec:
    def tests = JsonOutput.prettyPrint(JsonOutput.toJson(test_cases))
    def messages=[
            ["role": "system", "content": system_gen_system_prompt],
            ["role": "user", "content": "Here are some test cases:${tests}`\n\nHere is the description of the use-case: `${description.trim()}`\n\nRespond with your prompt, and nothing else. Be creative."]
    ]

    prompts = gptPromptForText(
                messages,
                model: params.CANDIDATE_MODEL,
                temperature: params.CANDIDATE_MODEL_TEMPERATURE,
                numOfChoices: number_of_prompts )
}

process test_candidate_prompt {
    input:
      val description
      tuple val(prompt1), val(prompt2)
      each test_case
    output:
      val result

    exec:
    def generation1 = get_generation(prompt1, test_case)
    def generation2 = get_generation(prompt2, test_case)

    // # Rank the outputs
    def result1 = get_score(description, test_case, generation1, generation2, params.RANKING_MODEL, params.RANKING_MODEL_TEMPERATURE)
    def result2 = get_score(description, test_case, generation2, generation1, params.RANKING_MODEL, params.RANKING_MODEL_TEMPERATURE)

    // # Convert scores to numeric values
    def score1 = result1 == 'A' ? 1 : (result1 == 'B' ? 0 : 0.5)
    def score2 = result2 == 'B' ? 1 : (result2 == 'A' ? 0 : 0.5)

    result = [score1:score1, score2:score2, prompt1:prompt1, prompt2:prompt2]
}


workflow generate_optimal_prompt {
    take:
      description
      test_cases
      number_of_prompts

    main:
      generate_candidate_prompts(description, test_cases, number_of_prompts) | flatMap{ combinations(it) } | set { prompt_pairs }
      test_candidate_prompt(description, prompt_pairs, test_cases) | reduce([:], this.&aggregate) | view { renderRatings(it) }
}

/*
 * entry point
 */
workflow {
    generate_optimal_prompt(description, test_cases, params.NUMBER_OF_PROMPTS)
}
